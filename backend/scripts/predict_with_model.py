# backend/scripts/predict_with_model.py

from datetime import datetime
import pandas as pd
import joblib
from pathlib import Path
from collections import Counter
import json
import matplotlib.pyplot as plt  

BASE_DIR = Path(__file__).resolve().parent.parent

def predict_churn(file_path: str, threshold: float = 0.5, versioned: bool = False):
    df = pd.read_csv(file_path)
    df['last_login'] = pd.to_datetime(df['last_login'])
    df['days_since_login'] = (pd.Timestamp.today() - df['last_login']).dt.days

    identity_df = df[['name', 'email', 'age', 'preferred_category']].copy()
    X = df[['age', 'watch_time', 'days_since_login']]

    scaler = joblib.load(BASE_DIR / "model/scaler.pkl")
    model = joblib.load(BASE_DIR / "model/model.pkl")

    X_scaled = scaler.transform(X)
    probs = model.predict_proba(X_scaled)[:, 1]

    print("ğŸ“Œ ì˜ˆì¸¡ í™•ë¥  ìƒìœ„ 10ê°œ:", probs[:10])
    print("ğŸ“Œ ì˜ˆì¸¡ í™•ë¥  ìµœì†Œ~ìµœëŒ€:", probs.min(), "~", probs.max())
    print("ğŸ“Œ ë°›ì€ threshold:", threshold)

    result_df = identity_df.copy()
    result_df['churn_probability'] = probs
    result_df['is_high_risk'] = result_df['churn_probability'] > threshold

    result_df.reset_index(drop=True, inplace=True)
    df.reset_index(drop=True, inplace=True)
    df["is_high_risk"] = result_df["is_high_risk"]

    print("ğŸ“Œ í™•ë¥  ìƒìœ„ 5ëª…:\n", result_df.sort_values(by="churn_probability", ascending=False).head())
    print("ğŸ“Œ is_high_risk True ê°œìˆ˜:", result_df['is_high_risk'].sum())

    churn_group = result_df[result_df["is_high_risk"]]
    non_churn_group = result_df[~result_df["is_high_risk"]]

    high_risk_df = df[df["is_high_risk"]].copy()
    high_risk_df["churn_probability"] = result_df.loc[high_risk_df.index, "churn_probability"].values
    high_risk_df = high_risk_df[[
        "name", "age", "last_login", "watch_time", "preferred_category", "email", "churn_probability"
    ]]
    high_risk_df["last_login"] = pd.to_datetime(high_risk_df["last_login"]).dt.strftime("%Y-%m-%d")
    high_risk_df["watch_time"] = pd.to_numeric(high_risk_df["watch_time"], errors="coerce").fillna(0).astype(int)
    high_risk_df["watch_time"] = high_risk_df["watch_time"].astype(str) + "ì‹œê°„"

    latest_path = BASE_DIR / "high_risk_customers.xlsx"
    high_risk_df.to_excel(latest_path, index=False)

    stats = {
        "total_customers": len(result_df),
        "expected_churns": int(result_df["is_high_risk"].sum()),
        "average_age": {
            "churn": round(churn_group["age"].mean(), 2) if not churn_group.empty else 0,
            "non_churn": round(non_churn_group["age"].mean(), 2) if not non_churn_group.empty else 0,
        },
        "average_watch_time": {
            "churn": round(df.loc[result_df["is_high_risk"], "watch_time"].mean(), 2) if not churn_group.empty else 0,
            "non_churn": round(df.loc[~result_df["is_high_risk"], "watch_time"].mean(), 2) if not non_churn_group.empty else 0,
        },
        "average_days_since_login": {
            "churn": round(df.loc[result_df["is_high_risk"], "days_since_login"].mean(), 2) if not churn_group.empty else 0,
            "non_churn": round(df.loc[~result_df["is_high_risk"], "days_since_login"].mean(), 2) if not non_churn_group.empty else 0,
        },
        "genre_distribution": dict(Counter(result_df["preferred_category"]))
    }

    stats_path = BASE_DIR.parent / "public" / "stats.json"
    stats_path.write_text(json.dumps(stats, ensure_ascii=False, indent=2))

    top_5 = result_df.sort_values(by="churn_probability", ascending=False).head()
    top_5.to_json(BASE_DIR / "top_5_customers.json", orient="records", force_ascii=False, indent=2)

    if versioned:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        versioned_filename = f"result_{timestamp}.xlsx"
        results_dir = BASE_DIR / "results"
        results_dir.mkdir(parents=True, exist_ok=True)
        versioned_path = results_dir / versioned_filename
        high_risk_df.to_excel(versioned_path, index=False)

        log_entry = {
            "csv_file": Path(file_path).name,
            "timestamp": timestamp,
            "threshold": threshold,
            "total_customers": stats["total_customers"],
            "expected_churns": stats["expected_churns"],
            "churn_rate": round(stats["expected_churns"] / stats["total_customers"], 3) if stats["total_customers"] > 0 else 0,
            "result_file": versioned_filename
        }

        logs_path = results_dir / "logs.json"
        try:
            if logs_path.exists() and logs_path.stat().st_size > 0:
                logs = json.loads(logs_path.read_text(encoding="utf-8"))
            else:
                logs = []
        except json.JSONDecodeError:
            print("âš ï¸ logs.json ê¹¨ì§ -> ìƒˆë¡œ ì´ˆê¸°í™”")
            logs = []

        logs.append(log_entry)
        logs_path.write_text(json.dumps(logs, ensure_ascii=False, indent=2))

        print(f"âœ… versioned ì‚¬ë³¸ ì €ì¥: {versioned_filename}")
        print("âœ… logs.json ê¸°ë¡ ì™„ë£Œ")

    print(f"âœ… ìµœì‹ ë³¸ ì €ì¥: {latest_path.name}")
    print("âœ… stats.json ì €ì¥ ì™„ë£Œ")
    print("âœ… top_5_customers.json ì €ì¥ ì™„ë£Œ")

    return result_df, churn_group, stats


"""
import pandas as pd
import joblib
from pathlib import Path
from collections import Counter
import json
import matplotlib.pyplot as plt

# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ backend ë””ë ‰í† ë¦¬ ê¸°ì¤€ ê²½ë¡œ í™•ë³´
BASE_DIR = Path(__file__).resolve().parent.parent  # backend/


def predict_churn(file_path: str, threshold: float = 0.5):
    # 1. ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(file_path)
    df['last_login'] = pd.to_datetime(df['last_login'])
    df['days_since_login'] = (pd.Timestamp.today() - df['last_login']).dt.days

    # ì‹ ì› ì •ë³´ ë°±ì—…
    identity_df = df[['name', 'email', 'age', 'preferred_category']].copy()

    # 2. ë¨¸ì‹ ëŸ¬ë‹ ì…ë ¥ìš© ì»¬ëŸ¼ ì„ íƒ
    X = df[['age', 'watch_time', 'days_since_login']]  # í›ˆë ¨ì— ì‚¬ìš©í•œ featureë§Œ

    # 3. ì •ê·œí™” ë° ì˜ˆì¸¡
    scaler = joblib.load(BASE_DIR/"model/scaler.pkl")
    model = joblib.load(BASE_DIR/"model/model.pkl")
    X_scaled = scaler.transform(X)
    probs = model.predict_proba(X_scaled)[:, 1]


    print("ğŸ“Œ ì˜ˆì¸¡ í™•ë¥  ìƒìœ„ 10ê°œ:", probs[:10])
    print("ğŸ“Œ ì˜ˆì¸¡ í™•ë¥  ìµœì†Œ~ìµœëŒ€:", probs.min(), "~", probs.max())


    # 4. ê²°ê³¼ ê²°í•©
    result_df = identity_df.copy()
    result_df['churn_probability'] = probs
    print("ğŸ“Œ ë°›ì€ threshold:", threshold)
    result_df['is_high_risk'] = result_df['churn_probability'] > threshold
    # ğŸ”¥ ë°˜ë“œì‹œ í•„ìš”! ì´í›„ df.loc[...]ì—ì„œ ì˜¬ë°”ë¥´ê²Œ ë™ì‘í•˜ê²Œ í•˜ê¸° ìœ„í•´
    result_df.reset_index(drop=True, inplace=True)
    df.reset_index(drop=True, inplace=True)
    df["is_high_risk"] = result_df["is_high_risk"]

    # ğŸ”¥ ë””ë²„ê·¸ ì¶œë ¥
    print("ğŸ“Œ í™•ë¥  ìƒìœ„ 5ëª…:\n", result_df.sort_values(by="churn_probability", ascending=False).head())
    print("ğŸ“Œ is_high_risk True ê°œìˆ˜:", result_df['is_high_risk'].sum())

    # 5. ê·¸ë£¹ë³„ ë¶„ë¦¬
    churn_group = result_df[result_df["is_high_risk"] == True]
    non_churn_group = result_df[result_df["is_high_risk"] == False]

    # 6. ì—‘ì…€ë¡œ ì €ì¥ (ì§€í‘œ ìˆœì„œëŒ€ë¡œ ì •ë¦¬)
    # 6. ì—‘ì…€ë¡œ ì €ì¥ (ì§€í‘œ ìˆœì„œëŒ€ë¡œ ì •ë¦¬)
    high_risk_mask = result_df["is_high_risk"]
    excel_df = df[high_risk_mask].copy()
    excel_df["churn_probability"] = result_df[high_risk_mask]["churn_probability"].values
    excel_df = excel_df[["name", "age", "last_login", "watch_time", "preferred_category", "email", "churn_probability"]]



    # âœ… í¬ë§· ë³€ê²½
    excel_df["last_login"] = pd.to_datetime(excel_df["last_login"]).dt.strftime("%Y-%m-%d")
    excel_df["watch_time"] = pd.to_numeric(excel_df["watch_time"], errors="coerce").fillna(0).astype(int)
    excel_df["watch_time"] = excel_df["watch_time"].astype(str) + "ì‹œê°„"

    # âœ… ì €ì¥
    excel_path = BASE_DIR / "high_risk_customers.xlsx"
    excel_df.to_excel(excel_path, index=False)

    # 7. í†µê³„ ë°ì´í„° ìƒì„±
    stats = {
        "total_customers": len(result_df),
        "expected_churns": int(result_df["is_high_risk"].sum()),

        "average_age": {
            "churn": round(churn_group["age"].mean(), 2) if not churn_group.empty else 0,
            "non_churn": round(non_churn_group["age"].mean(), 2) if not non_churn_group.empty else 0,
        },
        "average_watch_time": {
            "churn": round(df.loc[result_df["is_high_risk"], "watch_time"].mean(), 2) if not churn_group.empty else 0,
            "non_churn": round(df.loc[~result_df["is_high_risk"], "watch_time"].mean(), 2) if not non_churn_group.empty else 0,
        },
        "average_days_since_login": {
            "churn": round(df.loc[result_df["is_high_risk"], "days_since_login"].mean(), 2) if not churn_group.empty else 0,
            "non_churn": round(df.loc[~result_df["is_high_risk"], "days_since_login"].mean(), 2) if not non_churn_group.empty else 0,
        },
        "genre_distribution": dict(Counter(result_df["preferred_category"]))
    }

    # 8. email ë¦¬ìŠ¤íŠ¸ ì €ì¥
    email = result_df.sort_values(by="churn_probability", ascending=False).head()
    email.to_json("top_5_customers.json", orient="records", force_ascii=False, indent=2)

    # 8. stats.json ì €ì¥ (ReactTest-main/public)
    stats_path = BASE_DIR.parent / "public/stats.json"
    stats_path.parent.mkdir(parents=True, exist_ok=True)
    with open(stats_path, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    print("âœ… ì˜ˆì¸¡ ì™„ë£Œ: ê²°ê³¼ stats ìƒì„± ë° ì—‘ì…€ ì €ì¥ë¨")
    return result_df, churn_group, stats
"""

# import pandas as pd
# import joblib
# from pathlib import Path
# from collections import Counter
# import json
# import matplotlib.pyplot as plt

# # í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ backend ë””ë ‰í† ë¦¬ ê¸°ì¤€ ê²½ë¡œ í™•ë³´
# BASE_DIR = Path(__file__).resolve().parent.parent  # backend/


# def predict_churn(file_path: str, threshold: float = 0.5):
#     # 1. ë°ì´í„° ë¡œë“œ
#     df = pd.read_csv(file_path)
#     df['last_login'] = pd.to_datetime(df['last_login'])
#     df['days_since_login'] = (pd.Timestamp.today() - df['last_login']).dt.days

#     # ì‹ ì› ì •ë³´ ë°±ì—…
#     identity_df = df[['name', 'email', 'age', 'preferred_category']].copy()

#     # 2. ë¨¸ì‹ ëŸ¬ë‹ ì…ë ¥ìš© ì»¬ëŸ¼ ì„ íƒ
#     X = df[['age', 'watch_time', 'days_since_login']]  # í›ˆë ¨ì— ì‚¬ìš©í•œ featureë§Œ

#     # 3. ì •ê·œí™” ë° ì˜ˆì¸¡
#     scaler = joblib.load(BASE_DIR/"model/scaler.pkl")
#     model = joblib.load(BASE_DIR/"model/model.pkl")
#     X_scaled = scaler.transform(X)
#     probs = model.predict_proba(X_scaled)[:, 1]


#     print("ğŸ“Œ ì˜ˆì¸¡ í™•ë¥  ìƒìœ„ 10ê°œ:", probs[:10])
#     print("ğŸ“Œ ì˜ˆì¸¡ í™•ë¥  ìµœì†Œ~ìµœëŒ€:", probs.min(), "~", probs.max())


#     # 4. ê²°ê³¼ ê²°í•©
#     result_df = identity_df.copy()
#     result_df['churn_probability'] = probs
#     print("ğŸ“Œ ë°›ì€ threshold:", threshold)
#     result_df['is_high_risk'] = result_df['churn_probability'] > threshold
#     # ğŸ”¥ ë°˜ë“œì‹œ í•„ìš”! ì´í›„ df.loc[...]ì—ì„œ ì˜¬ë°”ë¥´ê²Œ ë™ì‘í•˜ê²Œ í•˜ê¸° ìœ„í•´
#     result_df.reset_index(drop=True, inplace=True)
#     df.reset_index(drop=True, inplace=True)
#     df["is_high_risk"] = result_df["is_high_risk"]

#     # ğŸ”¥ ë””ë²„ê·¸ ì¶œë ¥
#     print("ğŸ“Œ í™•ë¥  ìƒìœ„ 5ëª…:\n", result_df.sort_values(by="churn_probability", ascending=False).head())
#     print("ğŸ“Œ is_high_risk True ê°œìˆ˜:", result_df['is_high_risk'].sum())

#     # 5. ê·¸ë£¹ë³„ ë¶„ë¦¬
#     churn_group = result_df[result_df["is_high_risk"] == True]
#     non_churn_group = result_df[result_df["is_high_risk"] == False]

#     # 6. ì—‘ì…€ë¡œ ì €ì¥ (ì§€í‘œ ìˆœì„œëŒ€ë¡œ ì •ë¦¬)
#     # 6. ì—‘ì…€ë¡œ ì €ì¥ (ì§€í‘œ ìˆœì„œëŒ€ë¡œ ì •ë¦¬)
#     high_risk_mask = result_df["is_high_risk"]
#     excel_df = df[high_risk_mask].copy()
#     excel_df["churn_probability"] = result_df[high_risk_mask]["churn_probability"].values
#     excel_df = excel_df[["name", "age", "last_login", "watch_time", "preferred_category", "email", "churn_probability"]]



#     # âœ… í¬ë§· ë³€ê²½
#     excel_df["last_login"] = pd.to_datetime(excel_df["last_login"]).dt.strftime("%Y-%m-%d")
#     excel_df["watch_time"] = pd.to_numeric(excel_df["watch_time"], errors="coerce").fillna(0).astype(int)
#     excel_df["watch_time"] = excel_df["watch_time"].astype(str) + "ì‹œê°„"

#     # âœ… ì €ì¥
#     excel_path = BASE_DIR / "high_risk_customers.xlsx"
#     excel_df.to_excel(excel_path, index=False)

#     # 7. í†µê³„ ë°ì´í„° ìƒì„±
#     stats = {
#         "total_customers": len(result_df),
#         "expected_churns": int(result_df["is_high_risk"].sum()),

#         "average_age": {
#             "churn": round(churn_group["age"].mean(), 2) if not churn_group.empty else 0,
#             "non_churn": round(non_churn_group["age"].mean(), 2) if not non_churn_group.empty else 0,
#         },
#         "average_watch_time": {
#             "churn": round(df.loc[result_df["is_high_risk"], "watch_time"].mean(), 2) if not churn_group.empty else 0,
#             "non_churn": round(df.loc[~result_df["is_high_risk"], "watch_time"].mean(), 2) if not non_churn_group.empty else 0,
#         },
#         "average_days_since_login": {
#             "churn": round(df.loc[result_df["is_high_risk"], "days_since_login"].mean(), 2) if not churn_group.empty else 0,
#             "non_churn": round(df.loc[~result_df["is_high_risk"], "days_since_login"].mean(), 2) if not non_churn_group.empty else 0,
#         },
#         "genre_distribution": dict(Counter(result_df["preferred_category"]))
#     }

#     # 8. stats.json ì €ì¥ (ReactTest-main/public)
#     stats_path = BASE_DIR.parent / "public/stats.json"
#     stats_path.parent.mkdir(parents=True, exist_ok=True)
#     with open(stats_path, "w", encoding="utf-8") as f:
#         json.dump(stats, f, ensure_ascii=False, indent=2)

#     print("âœ… ì˜ˆì¸¡ ì™„ë£Œ: ê²°ê³¼ stats ìƒì„± ë° ì—‘ì…€ ì €ì¥ë¨")
#     return result_df, churn_group, stats

